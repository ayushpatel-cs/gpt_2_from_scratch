{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3c6fc5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F   \n",
    "import numpy as np     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0731d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mini_shakespeare.txt') as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "alphabet = sorted(list(set(file_content)))\n",
    "stoi = {char: i for i, char in enumerate(alphabet)}\n",
    "itos = {i: char for i, char in enumerate(alphabet)}\n",
    "encode = lambda x: [stoi[char] for char in x]\n",
    "decode = lambda x: [itos[encoded_char] for encoded_char in x]\n",
    "data = torch.tensor(encode(file_content), dtype=torch.long)\n",
    "cutoff = int(len(data)*0.9)\n",
    "train = data[:cutoff]\n",
    "val = data[cutoff:]\n",
    "\n",
    "vocab_size = len(alphabet)\n",
    "block_size = 8\n",
    "batch_size = 32\n",
    "embed_dim = 64\n",
    "num_heads = 4 ## must be divsor of embed_dim\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train if split == \"train\" else val   \n",
    "    batch_indicies = torch.randint(len(data) - block_size, (batch_size,1))\n",
    "    x = torch.stack([data[i:i+block_size] for i in batch_indicies])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in batch_indicies]) \n",
    "    return x, y\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.queries = nn.Linear(embed_dim, head_size, bias = False)\n",
    "        self.keys = nn.Linear(embed_dim, head_size, bias = False)\n",
    "        self.value_down = nn.Linear(embed_dim, head_size, bias = False)\n",
    "        self.head_size = head_size\n",
    "    def forward(self, x):\n",
    "        q = self.queries(x) ## (B, T, 32)\n",
    "        k = self.keys(x).transpose(1,2) ## (B, 32, T)\n",
    "        trans = q @ k /np.sqrt(self.head_size) ## (B, T, T)  \n",
    "        censored = torch.masked_fill(trans, ~torch.tril(torch.ones((batch_size, block_size, block_size))).bool(), float(\"-inf\"))\n",
    "        softmax = torch.softmax(censored, dim=2) ## (B, Querys, Keys)\n",
    "        attention = softmax @ self.value_down(x) ## (B, T, T) @ (B, T, 32)\n",
    "        return attention\n",
    "\n",
    "class MultiHeaded(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.value_up = nn.Linear(head_size*num_heads, embed_dim, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        concatenated_value_downs = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        return self.value_up(concatenated_value_downs)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embeding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.multiheaded_attention = MultiHeaded(num_heads, embed_dim//num_heads)\n",
    "        self.final = nn.Linear(embed_dim, vocab_size)  # final logits over vocab\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        logits = self.token_embeding(x)\n",
    "        logits = self.multiheaded_attention(logits)\n",
    "        logits = self.final(logits)  # final logits over vocab\n",
    "        \n",
    "        loss = None\n",
    "        if y != None:\n",
    "            B, T, C = logits.shape\n",
    "            x_reshaped = logits.view(B*T, C)\n",
    "            y_reshaped = y.view(B*T)\n",
    "            loss = F.cross_entropy(x_reshaped, y_reshaped)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, context, max_tokens = 100):\n",
    "        for i in range(max_tokens):\n",
    "            x = context[:, -block_size:]\n",
    "            logits, _ = self(x) ## (B, T, C)\n",
    "            logits = logits[:, -1, :] ## (B, C) last token\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1) # gets next token\n",
    "            context = torch.concat((context, next_token[:1]), dim=1)\n",
    "\n",
    "        return  context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "04d7d179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 4.196011066436768\n",
      "1000: 2.2246246337890625\n",
      "2000: 2.2780117988586426\n",
      "3000: 2.2658772468566895\n",
      "4000: 2.130584955215454\n",
      "tensor(2.3902, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "training_steps = 5000\n",
    "model = Transformer()\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "for _ in range(training_steps):\n",
    "    x, y = get_batch('train')\n",
    "    logits, loss = model(x,y)\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if _ % 1000 == 0:\n",
    "        print(f\"{_}: {loss}\")\n",
    "print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "93ace7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI MY NAME ISTHENTERTIIf Ifithalave rd.\n",
      "Werne Phalelleol wed,\n",
      "Thilon.\n",
      "\n",
      "AUMIFor, Anof her f b.\n",
      "\n",
      "Lilch wor fald th \n"
     ]
    }
   ],
   "source": [
    "# start with a context of just one token, e.g. the index for \"H\"\n",
    "start = torch.tensor([encode(\"HI MY NAME IS\")], dtype=torch.long)\n",
    "\n",
    "# generate 100 new tokens\n",
    "out = model.generate(start, max_tokens=100)\n",
    "\n",
    "# convert indices back to characters\n",
    "generated_text = ''.join([itos[int(i)] for i in out[0]])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da4791e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a31b4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_hw4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
